{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%EA%B0%80%EB%82%98%EB%8B%A4ABC123\n",
      "%EA%B0%80%EB%82%98%EB%8B%A4ABC123\n",
      "%EA%B0%80%EB%82%98%EB%8B%A4%20ABC123\n",
      "%EA%B0%80%EB%82%98%EB%8B%A4+ABC123\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "print(urllib.parse.quote('가나다ABC123')) # URL로 이동하기 위한 쿼리 문자열을 만들 때 \n",
    "print(urllib.parse.quote_plus('가나다ABC123'))\n",
    "print(urllib.parse.quote('가나다 ABC123')) # 중간에 blanck(빈칸) 있는 경우에는\n",
    "print(urllib.parse.quote_plus('가나다 ABC123')) # 서로 결과 다름! 공백이 있는 경우에는 일반적으로 urllib.parse.quote_plus 쓴다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "q=%EA%B0%80%EB%82%98%EB%8B%A4\n",
      "q=%EA%B0%80%EB%82%98%EB%8B%A4\n",
      "q=%EA%B0%80%EB%82%98%EB%8B%A4\n"
     ]
    }
   ],
   "source": [
    "query_str1 = \"q=\"+urllib.parse.quote('가나다') # 셋 다 결과 동일\n",
    "query_str2 = \"q=\"+urllib.parse.quote_plus('가나다')\n",
    "query_str3 = urllib.parse.urlencode({\"q\" : \"가나다\"})\n",
    "print(\"-------------------------------------------\")\n",
    "print(query_str1)\n",
    "print(query_str2)\n",
    "print(query_str3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 360번 버스의 운행 위치 ]\n",
      "360번 버스 91대 운행중...\n",
      "37.4699297672,127.1275897549\n",
      "37.4790433896,127.1261237834\n",
      "37.482125,127.124748\n",
      "37.4883648963,127.1213627915\n",
      "37.4943822012,127.1171609467\n",
      "37.5002084826,127.1118384337\n",
      "37.5034082968,127.1089442853\n",
      "37.5068001738,127.1058262006\n",
      "37.5128993133,127.0979457604\n",
      "37.5122132454,127.0939144028\n",
      "37.5117722886,127.0872805398\n",
      "37.5118706155,127.0833904794\n",
      "37.512009235,127.0796916235\n",
      "37.5111175337,127.0720732937\n",
      "37.5099462683,127.0657996068\n",
      "37.5086670057,127.0617005963\n",
      "37.5066831874,127.0554269279\n",
      "37.505143,127.050205\n",
      "37.503232,127.044057\n",
      "37.5015409767,127.0386328674\n",
      "37.5004418394,127.0350786317\n",
      "37.4988294041,127.0298390986\n",
      "37.500785,127.02637\n",
      "37.506367,127.023728\n",
      "37.5106480618,127.0192203041\n",
      "37.5083418804,127.0121292217\n",
      "37.5062923789,127.0052834138\n",
      "37.5034152049,126.9958263651\n",
      "37.5019451598,126.9900264523\n",
      "37.503188597,126.9766525988\n",
      "37.505867,126.971748\n",
      "37.5064370015,126.9685844969\n",
      "37.5075850724,126.9655921789\n",
      "37.5115728985,126.9612375315\n",
      "37.512734203,126.9509621786\n",
      "37.5138325876,126.9447114504\n",
      "37.5134800797,126.9388655754\n",
      "37.5132368488,126.9323737344\n",
      "37.5130048235,126.9242224384\n",
      "37.5140830291,126.9195131812\n",
      "37.516815,126.916956\n",
      "37.5218906273,126.9188836862\n",
      "37.5254720914,126.9241508208\n",
      "37.5269560563,126.9270250084\n",
      "37.5273043795,126.9316210834\n",
      "37.5259920176,126.9309977447\n",
      "37.5250045778,126.9281836353\n",
      "37.5260593379,126.9246697181\n",
      "37.523605,126.920262\n",
      "37.5183978581,126.9148883495\n",
      "37.5167591036,126.9166368877\n",
      "37.5136409433,126.9200351486\n",
      "37.512907,126.923495\n",
      "37.513174,126.931437\n",
      "37.513423,126.938456\n",
      "37.513733,126.943811\n",
      "37.512825,126.949256\n",
      "37.5130085949,126.9567458811\n",
      "37.511297,126.96101\n",
      "37.5074984059,126.9650447528\n",
      "37.5063016864,126.9680257853\n",
      "37.5054978005,126.9711242049\n",
      "37.5024209111,126.9769231591\n",
      "37.5016386844,126.9888988467\n",
      "37.5029429707,126.9944510234\n",
      "37.5057382586,127.003792583\n",
      "37.5079040519,127.0110398687\n",
      "37.510420605,127.0196823939\n",
      "37.5072264964,127.0230319441\n",
      "37.501817,127.025438\n",
      "37.4981357274,127.0291477897\n",
      "37.4995717499,127.0337900061\n",
      "37.501137,127.038818\n",
      "37.502783,127.044247\n",
      "37.5047611955,127.050462265\n",
      "37.506043,127.054803\n",
      "37.5081065399,127.0616639386\n",
      "37.5095036021,127.0660087348\n",
      "37.5104187252,127.0728158016\n",
      "37.5116128425,127.0795554593\n",
      "37.5114066648,127.0844951326\n",
      "37.5115116112,127.0912020656\n",
      "37.51241161,127.0981962498\n",
      "37.507105259,127.1052520216\n",
      "37.5036628588,127.108453771\n",
      "37.5005992316,127.111232788\n",
      "37.4947387984,127.1164919903\n",
      "37.48901,127.120775\n",
      "37.48309,127.123873\n",
      "37.479907,127.125472\n",
      "37.470728,127.128144\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "busNum = '360'\n",
    "key = '%2BjzsSyNtwmcqxUsGnflvs3rW2oceFvhHR8AFkM3ao%2Fw50hwHXgGyPVutXw04uAXvrkoWgkoScvvhlH7jgD4%2FRQ%3D%3D'\n",
    "url1 = 'http://ws.bus.go.kr/api/rest/busRouteInfo/getBusRouteList?serviceKey='+key+'&strSrch='+busNum\n",
    "res = req.urlopen(url1)\n",
    "\n",
    "soup = BeautifulSoup(res.read().decode('utf-8'), features=\"xml\") # xml은 반드시 utf-8로!!\n",
    "\n",
    "busRouteId = None\n",
    "for itemList in soup.find_all('itemList') :\n",
    "    busRouteId = itemList.find('busRouteId').string\n",
    "    busRouteNm = itemList.find('busRouteNm').string\n",
    "    if busRouteNm == busNum :\n",
    "        break\n",
    "\n",
    "url2 = 'http://ws.bus.go.kr/api/rest/busRouteInfo/getStaionByRoute?ServiceKey='+key+'&busRouteId='+busRouteId\n",
    "res = req.urlopen(url2)\n",
    "soup = BeautifulSoup(res.read().decode('utf-8'), features=\"xml\")\n",
    "\n",
    "busPos = []\n",
    "for itemList in soup.find_all('itemList') :\n",
    "    gpsY = itemList.find('gpsY').string\n",
    "    gpsX = itemList.find('gpsX').string\n",
    "\n",
    "    busPos.append((gpsY, gpsX))\n",
    "\n",
    "print('[ ' + busNum + '번 버스의 운행 위치 ]')\n",
    "if len(busPos) != 0 :\n",
    "    print(busNum + '번 버스 ' + str(len(busPos)) + '대 운행중...')\n",
    "    for lat,lng in busPos :\n",
    "        print(lat+','+lng)\n",
    "else :\n",
    "    print('현재 운행중인 ' + busNum + '번 버스가 없어요...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "key = '796143536a756e69313134667752417a'\n",
    "contentType = 'xml'\n",
    "startIndex = '1'\n",
    "endIndex = '100'\n",
    "url = 'http://openapi.seoul.go.kr:8088/'+key+'/'+contentType+'/LampScpgmtb/'+startIndex+'/'+endIndex+'/'\n",
    "savename = 'output/edu.xml'\n",
    "req.urlretrieve(url, savename)\n",
    "\n",
    "xml = open(savename, 'r', encoding='utf-8').read()\n",
    "soup = BeautifulSoup(xml, 'xml')\n",
    "\n",
    "pjList = []\n",
    "for itemList in soup.find_all('row') :\n",
    "    up_nm = itemList.find('UP_NM').string\n",
    "    up_nm = '없음' if up_nm is None else up_nm\n",
    "    pgm_nm = itemList.find('PGM_NM').string\n",
    "    pgm_nm = '없음' if pgm_nm is None else pgm_nm\n",
    "    target_nm = itemList.find('TARGET_NM').string\n",
    "    target_nm = '없음' if target_nm is None else target_nm\n",
    "    u_price = itemList.find('U_PRICE').string\n",
    "    u_price = '없음' if u_price is None else u_price\n",
    "    pjList.append((up_nm, pgm_nm, target_nm, u_price))\n",
    "\n",
    "print('[ 서울 청소년 수련관 강좌 리스트 ]')\n",
    "for up_nm,pgm_nm,target_nm,u_price in pjList :\n",
    "    print(up_nm+','+pgm_nm+','+target_nm+','+str(u_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import io\n",
    "\n",
    "key = '796143536a756e69313134667752417a'\n",
    "contentType = 'xml'\n",
    "startIndex = '1'\n",
    "endIndex = '100'\n",
    "date = '20201010'\n",
    "\n",
    "url = 'http://openapi.seoul.go.kr:8088/'+key+'/'+contentType+'/CardSubwayStatsNew/'+startIndex+'/'+endIndex+'/'+date+'/'\n",
    "savename = 'output/subway.xml'\n",
    "req.urlretrieve(url, savename)\n",
    "\n",
    "xml = open(savename, 'r', encoding='utf-8').read()\n",
    "soup = BeautifulSoup(xml, 'xml')\n",
    "\n",
    "subwayList = []\n",
    "for itemList in soup.find_all('row') :\n",
    "    line_num = itemList.find('LINE_NUM').string\n",
    "    sub_sta_nm = itemList.find('SUB_STA_NM').string\n",
    "    ride_pasgr_num = itemList.find('RIDE_PASGR_NUM').string\n",
    "    alight_pasgr_num = itemList.find('ALIGHT_PASGR_NUM').string\n",
    "    subwayList.append((line_num, sub_sta_nm, ride_pasgr_num, alight_pasgr_num))\n",
    "\n",
    "print('[ 서울시 지하철호선별 역별 승하차 인원 정보 ]')\n",
    "for line_num, sub_sta_nm, ride_pasgr_num, alight_pasgr_num in subwayList :\n",
    "    print(line_num+','+sub_sta_nm+','+ride_pasgr_num+','+alight_pasgr_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import io\n",
    "\n",
    "key = '4WQ7X833TXC370SUTDX4'\n",
    "contentType = 'xml'\n",
    "startIndex = '1'\n",
    "endIndex = '100'\n",
    "\n",
    "url = 'http://ecos.bok.or.kr/api/KeyStatisticList/'+key+'/'+contentType+'/kr/'+startIndex+'/'+endIndex+'/'\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res.read().decode('utf-8'), 'xml')\n",
    "ecoList = []\n",
    "for itemList in soup.find_all('row') :\n",
    "    class_name = itemList.find('CLASS_NAME').string\n",
    "    class_name = '없음' if class_name is None else class_name\n",
    "    keystat_name = itemList.find('KEYSTAT_NAME').string\n",
    "    keystat_name = '없음' if keystat_name is None else keystat_name\n",
    "    data_value = itemList.find('DATA_VALUE').string\n",
    "    data_value = '없음' if data_value is None else data_value\n",
    "    cycle = itemList.find('CYCLE').string\n",
    "    cycle = '없음' if cycle is None else cycle\n",
    "    unit_name = itemList.find('UNIT_NAME').string\n",
    "    unit_name = '없음' if unit_name is None else unit_name\n",
    "    ecoList.append((class_name, keystat_name, data_value, cycle, unit_name))\n",
    "\n",
    "print('[ 100대 통계 지표 ]')\n",
    "for class_name, keystat_name, data_value, cycle, unit_name in ecoList :\n",
    "    print(class_name+','+keystat_name+','+data_value+','+cycle+','+unit_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = '수능'\n",
    "encText = urllib.parse.quote_plus(query)\n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/blog.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('[' + query + '에 대한 네이버 블로그 글 ]')\n",
    "    for data in dataList['items'] :\n",
    "        print (str(count) + ' : ' + data['title'])\n",
    "        print ('[' + data['description'] + ']')\n",
    "        count += 1\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = 'tns'\n",
    "encText = urllib.parse.quote_plus(query)\n",
    "\n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/news.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "rescode = response.getcode()\n",
    "\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('[' + query + '에 대한 네이버 뉴스 글 ]')\n",
    "    for data in dataList['items'] :\n",
    "        print (str(count) + ' : ' + data['title'])\n",
    "        print ('[' + data['description'] + ']')\n",
    "        count += 1\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "api_key = \"RvnZeIl8ra88reu8fm23m0bST\"\n",
    "api_secret = \"wTRylK94GK2KmhZUnqXonDaIszwAsS6VPvpSsIo6EX5GQLtzQo\"\n",
    "access_token = \"959614462004117506-dkWyZaO8Bz3ZXh73rspWfc1sQz0EnDU\"\n",
    "access_token_secret = \"rxDWfg7uz1yXMTDwijz0x90yWhDAnmOM15R6IgC8kmtTe\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "keyword = '치맥'   \n",
    "search = []   \n",
    "\n",
    "cnt = 1\n",
    "while(cnt <= 10):   \n",
    "    tweets = api.search(q=keyword, count=100)\n",
    "    for tweet in tweets :\n",
    "        search.append(tweet)\n",
    "    cnt += 1\n",
    "\n",
    "data = {}   \n",
    "i = 1   \n",
    "print('[' + keyword + '에 대한 트윗 글 ]')    \n",
    "for tweet in search:\n",
    "    data['text'] = tweet.text   \n",
    "    print(i, \" : \", data)   \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "client_id = '0LHQM4VX_MQM6JfkXofa'\n",
    "client_secret = 'OcPgqpswCg'\n",
    "\n",
    "\n",
    "#[CODE 1]\n",
    "def getRequestUrl(url):    \n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    req.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    \n",
    "    try: \n",
    "        response = urllib.request.urlopen(req)\n",
    "        if response.getcode() == 200:\n",
    "            print (\"[%s] Url Request Success\" % datetime.datetime.now())\n",
    "            return response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"[%s] Error for URL : %s\" % (datetime.datetime.now(), url))\n",
    "        return None\n",
    "\n",
    "#[CODE 2]\n",
    "def getNaverSearch(node, srcText, start, display):    \n",
    "    base = \"https://openapi.naver.com/v1/search\"\n",
    "    node = \"/%s.json\" % node\n",
    "    parameters = \"?query=%s&start=%s&display=%s\" % (urllib.parse.quote(srcText), start, display)\n",
    "    \n",
    "    url = base + node + parameters    \n",
    "    responseDecode = getRequestUrl(url)   #[CODE 1]\n",
    "    \n",
    "    if (responseDecode == None):\n",
    "        return None\n",
    "    else:\n",
    "        return json.loads(responseDecode)\n",
    "\n",
    "#[CODE 3]\n",
    "def getPostData(post, jsonResult, cnt):    \n",
    "    title = post['title']\n",
    "    description = post['description']\n",
    "    org_link = post['originallink']\n",
    "    link = post['link']\n",
    "    \n",
    "    pDate = datetime.datetime.strptime(post['pubDate'],  '%a, %d %b %Y %H:%M:%S +0900')\n",
    "    pDate = pDate.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    jsonResult.append({'cnt':cnt, 'title':title, 'description': description, \n",
    "'org_link':org_link,   'link': org_link,   'pDate':pDate})\n",
    "    return    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어를 입력하세요:  황사\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:36.203895] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:36.372907] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:36.552920] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:36.717933] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:36.890946] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:37.074960] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:37.258973] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:37.448988] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:37.630001] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:37.818015] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "HTTP Error 400: Bad Request\n",
      "[2021-03-29 12:35:37.884021] Error for URL : https://openapi.naver.com/v1/search/news.json?query=%ED%99%A9%EC%82%AC&start=1001&display=100\n",
      "전체 검색 : 195273 건\n",
      "가져온 데이터 : 1000 건\n",
      "output/황사_naver_news.json SAVED\n"
     ]
    }
   ],
   "source": [
    "node = 'news'   # 크롤링 할 대상\n",
    "srcText = input('검색어를 입력하세요: ')\n",
    "cnt = 0\n",
    "jsonResult = []\n",
    "\n",
    "jsonResponse = getNaverSearch(node, srcText, 1, 100)  #[CODE 2]\n",
    "total = jsonResponse['total']\n",
    " \n",
    "while ((jsonResponse != None) and (jsonResponse['display'] != 0)):         \n",
    "    for post in jsonResponse['items']:\n",
    "        cnt += 1\n",
    "        getPostData(post, jsonResult, cnt)  #[CODE 3]       \n",
    "        \n",
    "    start = jsonResponse['start'] + jsonResponse['display']\n",
    "    jsonResponse = getNaverSearch(node, srcText, start, 100)  #[CODE 2]\n",
    "       \n",
    "print('전체 검색 : %d 건' %total)\n",
    "    \n",
    "with open('%s_naver_%s.json' % (srcText, node), 'w', encoding='utf8') as outfile:\n",
    "    jsonFile = json.dumps(jsonResult,  indent=4, sort_keys=True,  ensure_ascii=False)\n",
    "                        \n",
    "    outfile.write(jsonFile)\n",
    "        \n",
    "print(\"가져온 데이터 : %d 건\" %(cnt))\n",
    "print ('output/%s_naver_%s.json SAVED' % (srcText, node))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#[CODE 1]\n",
    "def hollys_store(result):\n",
    "    for page in range(1,59):\n",
    "        Hollys_url = 'https://www.hollys.co.kr/store/korea/korStore.do?pageNo=%d&sido=&gugun=&store=' %page\n",
    "        print(Hollys_url)\n",
    "        html = urllib.request.urlopen(Hollys_url)\n",
    "        soupHollys = BeautifulSoup(html, 'html.parser')\n",
    "        tag_tbody = soupHollys.find('tbody')\n",
    "        for store in tag_tbody.find_all('tr'):\n",
    "            if len(store) <= 3:\n",
    "                break\n",
    "            store_td = store.find_all('td')\n",
    "            store_name = store_td[1].string\n",
    "            store_sido = store_td[0].string\n",
    "            store_address = store_td[3].string\n",
    "            store_phone = store_td[5].string\n",
    "            result.append([store_name]+[store_sido]+[store_address]\n",
    "                          +[store_phone])\n",
    "    return\n",
    "\n",
    "#[CODE 0]\n",
    "result = []\n",
    "print('Hollys store crawling >>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "hollys_store(result)   #[CODE 1] 호출 \n",
    "hollys_tbl = pd.DataFrame(result, columns=('store', 'sido-gu', 'address','phone'))\n",
    "hollys_tbl.to_csv('output/hollys.csv', encoding='cp949', mode='w', index=True)\n",
    "del result[:]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "#[CODE 1]\n",
    "def CoffeeBean_store(result):\n",
    "    CoffeeBean_URL = \"https://www.coffeebeankorea.com/store/store.asp\"\n",
    "    wd = webdriver.Chrome('c:/Temp/chromedriver.exe')\n",
    "             \n",
    "    for i in range(1, 370):  #매장 수 만큼 반복\n",
    "        wd.get(CoffeeBean_URL)\n",
    "        time.sleep(1)  #웹페이지 연결할 동안 1초 대기\n",
    "        try:\n",
    "            wd.execute_script(\"storePop2(%d)\" %i)\n",
    "            time.sleep(1) #스크립트 실행 할 동안 1초 대기\n",
    "            html = wd.page_source\n",
    "            soupCB = BeautifulSoup(html, 'html.parser')\n",
    "            store_name_h2 = soupCB.select(\"div.store_txt > h2\")\n",
    "            store_name = store_name_h2[0].string\n",
    "            print(store_name)  #매장 이름 출력하기\n",
    "            store_info = soupCB.select(\"div.store_txt > table.store_table > tbody > tr > td\")\n",
    "            store_address_list = list(store_info[2])\n",
    "            store_address = store_address_list[0]\n",
    "            store_phone = store_info[3].string\n",
    "            result.append([store_name]+[store_address]+[store_phone])\n",
    "        except:\n",
    "            continue \n",
    "    return\n",
    "\n",
    "result = []\n",
    "print('CoffeeBean store crawling >>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "CoffeeBean_store(result)  #[CODE 1]\n",
    "    \n",
    "CB_tbl = pd.DataFrame(result, columns=('store', 'address','phone'))\n",
    "CB_tbl.to_csv('output/CoffeeBean.csv', encoding='cp949', mode='w', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydatavenv",
   "language": "python",
   "name": "pydatavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
